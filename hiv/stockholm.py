import logging
import networkx as nx
import numpy as np
from .util import to_np_dict


LOGGER = logging.getLogger(__name__)
Edge = tuple[int, int]


def get_edges(graph: nx.Graph, casual: bool) -> list[Edge]:
    """
    Get casual or steady edges.
    """
    return [(u, v) for u, v, data in graph.edges(data=True) if data["is_casual"] == casual]


def evaluate_num_edges(graph: nx.Graph, casual: bool) -> int:
    """
    Evaluate the number of casual or steady edges.
    """
    return len(get_edges(graph, casual))


def get_nodes(graph: nx.Graph, single: bool) -> int:
    """
    Get single or paired nodes.
    """
    return [node for node, data in graph.nodes(data=True) if data["is_single"] == single]


def evaluate_num_nodes(graph: nx.Graph, single: bool) -> int:
    """
    Evaluate the number of single or partnered nodes.
    """
    return len(get_nodes(graph, single))


def add_edges_from_candidates(graph: nx.Graph, candidates: np.ndarray, **kwargs) -> np.ndarray:
    """
    Add random edges from a list of candidate nodes by pairing them. Existing edges are left
    unchanged (important for not accidentally turning steady relationships into casual ones).

    Args:
        graph: Graph to add edges to.
        candidates: Set of candidates to pair up.
        **kwargs: Common properties for all edges.

    Returns:
        edges: List of edges.
    """
    candidates = np.random.permutation(candidates)
    num_edges = candidates.size // 2
    candidates = candidates[:2 * num_edges]
    edges = candidates.reshape((num_edges, 2))
    # Filter to edges that do not already exist.
    edges = np.asarray([edge for edge in edges if not graph.has_edge(*edge)])
    graph.add_edges_from(edges, **kwargs)
    return edges


def _maybe_verify_state(verify: bool, graph: nx.Graph, num_steady_edges: int) -> None:
    if not verify:
        return
    # Verify number of edges of each type.
    assert evaluate_num_edges(graph, False) == num_steady_edges
    assert graph.number_of_edges() == num_steady_edges + evaluate_num_edges(graph, True)

    # Verify status flags for each node.
    for node, data in graph.nodes(data=True):
        # Nodes that are single should have zero edges that are not casual. Nodes that are paired
        # should have exactly one edge that is not casual.
        single_degree = sum(not edge_data["is_casual"] for *_, edge_data in
                            graph.edges(node, data=True))
        assert (data["is_single"] and single_degree == 0) \
            or (not data["is_single"] and single_degree == 1)
        # Nodes that have a casual relationship should have exactly one of them.
        casual_degree = sum(edge_data["is_casual"] for *_, edge_data in
                            graph.edges(node, data=True))
        assert (data["has_casual"] and casual_degree == 1) \
            or (not data["has_casual"] and casual_degree == 0)


def simulate(n: float, mu: float, sigma: float, rho: float, w0: float, w1: float, num_steps: int,
             graph: nx.Graph = None, step: int = 0, verify: bool = False) -> tuple[nx.Graph, dict]:
    """
    Simulate the Stockholm model.

    Args:
        n: Expected number of nodes.
        mu: Probability for a node to leave the population.
        sigma: Probability for a steady relationship to end.
        rho: Probability for a single node to consider a steady relationship.
        w0: Probability for a single node to consider a casual contact.
        w1: Probability for a partnered node to consider a casual contact.
        num_steps: Number of iterations.
        graph: Initial graph that is modified in-place.
        step: Step index (required to evaluate the length of steady relationships if graphs are
            generated by calling the function multiple times).

    Returns:
        graph: Graph after simulating for `num_steps` iterations.
        statistics: Sequence of summary statistics.
    """
    graph = nx.Graph() if graph is None else graph
    label_offset = max(graph, default=-1) + 1

    statistics = {}
    num_steady_edges = evaluate_num_edges(graph, casual=False)

    for step in step + np.arange(num_steps):
        # Identify nodes to be removed and evaluate the durations of steady relationships.
        nodes_to_remove = [node for node in graph if np.random.binomial(1, mu)]
        num_steady_removed = 0
        for *edge, data in graph.edges(nodes_to_remove, data=True):
            if data["is_casual"]:
                graph.add_nodes_from(edge, has_casual=False)
            else:
                duration = step - data["created_at"]
                statistics.setdefault("durations", []).append(duration)
                num_steady_edges -= 1
                graph.add_nodes_from(edge, is_single=True)
                num_steady_removed += 1
        graph.remove_nodes_from(nodes_to_remove)

        LOGGER.info("removed %d nodes with %d steady edges (total steady=%d)", len(nodes_to_remove),
                    num_steady_removed, num_steady_edges)
        _maybe_verify_state(verify, graph, num_steady_edges)

        # Delete all casual edges and remove steady edges with probability sigma.
        edges_to_remove = []
        durations = []
        for *edge, data in graph.edges(data=True):
            if data["is_casual"]:
                edges_to_remove.append(edge)
                graph.add_nodes_from(edge, has_casual=False)
            elif np.random.binomial(1, sigma):
                edges_to_remove.append(edge)
                durations.append(step - data["created_at"])
                graph.add_nodes_from(edge, is_single=True)
        graph.remove_edges_from(edges_to_remove)
        num_steady_edges -= len(durations)
        statistics.setdefault("durations", []).extend(durations)

        LOGGER.info("removed %d edges of which %d were steady (total steady=%d)",
                    len(edges_to_remove), len(durations), num_steady_edges)
        _maybe_verify_state(verify, graph, num_steady_edges)

        # Add new nodes.
        num_new_nodes = np.random.poisson(n * mu)
        graph.add_nodes_from(label_offset + np.arange(num_new_nodes), is_single=True,
                             has_casual=False)
        label_offset += num_new_nodes

        LOGGER.info("added %d nodes (total=%d)", num_new_nodes, graph.number_of_nodes())

        # Add steady relationships.
        singles = [node for node, data in graph.nodes(data=True) if data["is_single"]
                   and np.random.binomial(1, rho)]
        new_steady = add_edges_from_candidates(graph, singles, created_at=step, is_casual=False)
        graph.add_nodes_from(new_steady.ravel(), is_single=False)
        num_steady_edges += len(singles) // 2

        LOGGER.info("added %d steady edges (total steady=%d)", len(singles) // 2, num_steady_edges)
        _maybe_verify_state(verify, graph, num_steady_edges)

        # Add casual relationships.
        candidates = [node for node, data in graph.nodes(data=True) if
                      (data["is_single"] and np.random.binomial(1, w0)) or
                      (not data["is_single"] and np.random.binomial(1, w1))]
        new_casual = add_edges_from_candidates(graph, candidates, is_casual=True)
        graph.add_nodes_from(new_casual.ravel(), has_casual=True)
        num_casual_edges = len(candidates) // 2

        LOGGER.info("added %d casual edges", num_casual_edges)
        _maybe_verify_state(verify, graph, num_steady_edges)

        # Report statistics.
        statistics.setdefault("num_steady_edges", []).append(num_steady_edges)
        statistics.setdefault("num_casual_edges", []).append(num_casual_edges)

    return graph, to_np_dict(statistics)
